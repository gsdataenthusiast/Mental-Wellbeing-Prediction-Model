# -*- coding: utf-8 -*-
"""Model Selection and Tuning Trials.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19HxppCQPCdEssL24Mf29Q4mI0G9Kd5vZ
"""

# Commented out IPython magic to ensure Python compatibility.
##Load Require Libraries
import re
import numpy as np
import pandas as pd
import seaborn as sns
import missingno as msno
import matplotlib.pyplot as plt
# %matplotlib inline
import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
!pip install texthero  
import texthero as hero
from texthero import preprocessing
from texthero import stopwords
import plotly.express as px
from PIL import Image

# Import the training Data (Cleaned and processed)
from google.colab import files
data_to_load = files.upload()

# Import the generated Data
SAD_Subreddit_train_Data = pd.read_csv('SAD_Subreddit_train_Data.csv')

# Viewing a glimpse of the Data
SAD_Subreddit_train_Data.head()

SAD_Subreddit_train_Data = SAD_Subreddit_train_Data.drop(SAD_Subreddit_train_Data.columns[0],axis=1) ##Deleted unwanted Column
# Viewing a glimpse of the Data
SAD_Subreddit_train_Data.head()

feature = SAD_Subreddit_train_Data['Clean Text'].values
label = SAD_Subreddit_train_Data['category_code'].values

from sklearn.model_selection import train_test_split
trainX, testX, trainY, testY = train_test_split(feature, label, test_size=0.20, random_state= None)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

count_vect = CountVectorizer(ngram_range=(1,3),min_df=0,max_df=1.0)

X_train_counts = count_vect.fit_transform(trainX)
count_vect.vocabulary_

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

X_test_counts = count_vect.transform(testX)
count_vect.vocabulary_

X_test_tfidf = tfidf_transformer.transform(X_test_counts)

##logistic regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()

log_reg = lr.fit(X_train_tfidf, trainY)

predY = log_reg.predict(X_test_tfidf)

##Classification Report
from sklearn import metrics
print(metrics.classification_report(testY,predY))

##XGB Classifier
from xgboost import XGBClassifier
model = XGBClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)
model.fit(X_train_tfidf, trainY)

predY = model.predict(X_test_tfidf)
from sklearn import metrics
print(metrics.classification_report(testY,predY))

##knn
from sklearn import neighbors
knn = neighbors.KNeighborsClassifier(n_neighbors=5)

knn_clf = knn.fit(X_train_tfidf, trainY)

predY = log_reg.predict(X_test_tfidf)

##Classification Report
from sklearn import metrics
print(metrics.classification_report(testY,predY))

##Multinomial Naive Bayes
from sklearn.naive_bayes import MultinomialNB

MNB = MultinomialNB().fit(X_train_tfidf, y_train)

MNB = MNB.fit(X_train_tfidf, trainY)

predY = MNB.predict(X_test_tfidf)

##SVM Kernel Selection
##Justifying the Kernel selection of the SVM model
kernels = ['Polynomial', 'RBF', 'Sigmoid','Linear']#A function which returns the corresponding SVC model
def getClassifier(ktype):
    if ktype == 0:
        # Polynomial kernal
        return SVC(kernel='poly', degree=8, gamma="auto")
    elif ktype == 1:
        # Radial Basis Function kernal
        return SVC(kernel='rbf', gamma="auto")
    elif ktype == 2:
        # Sigmoid kernal
        return SVC(kernel='sigmoid', gamma="auto")
    elif ktype == 3:
        # Linear kernal
        return SVC(kernel='linear', gamma="auto")

from sklearn.metrics import classification_report
  for i in range(4):
    svclassifier = getClassifier(i) 
    svclassifier.fit(X_train_tfidf, y_train)# Make prediction
    y_pred = svclassifier.predict(count_vect.transform(X_test))# Evaluate our model
    print("Evaluation:", kernels[i], "kernel")
    print(classification_report(y_test,y_pred))

##gridsearch CV
from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['linear']}

grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2) ##Gridsearch to find the best parameters
grid.fit(X_train_tfidf,y_train)

## using only tf-idf and no count vector
# TFIDF function min_df = 1,
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(sublinear_tf= True, #use a logarithmic form for frequency
                       min_df = 1, #minimum numbers of documents a word must be present in to be kept
                       norm= 'l2', #ensure all our feature vectors have a euclidian norm of 1
                       ngram_range= (1,2), #to indicate that we want to consider both unigrams and bigrams.
                       stop_words ='english') #to remove all common pronouns to reduce the number of noisy features

features = tfidf.fit_transform(SAD_Subreddit_Data['Clean Text']).toarray()
labels = SAD_Subreddit_Data['Post Category']
features.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features,labels, test_size=0.20, random_state= 0 , stratify=labels)

from sklearn.svm import SVC
svc = SVC(kernel='linear')
svm = svc.fit(X_train, y_train)

y_pred = svm.predict(X_test)

##svm with unigrams
feature = SAD_Subreddit_train_Data['Clean Text'].values
label = SAD_Subreddit_train_Data['category_code'].values

from sklearn.model_selection import train_test_split
trainX, testX, trainY, testY = train_test_split(feature, label, test_size=0.20, random_state= None)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

count_vect = CountVectorizer(ngram_range=(1,1),min_df=0,max_df=1.0)

X_train_counts = count_vect.fit_transform(trainX)
count_vect.vocabulary_

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

X_test_counts = count_vect.transform(testX)
count_vect.vocabulary_

X_test_tfidf = tfidf_transformer.transform(X_test_counts)

from sklearn.svm import SVC
svc = SVC(kernel='linear')
svm = svc.fit(X_train, y_train)

y_pred = svm.predict(X_test)

##Classification Report
from sklearn import metrics
print(metrics.classification_report(testY,predY))