# -*- coding: utf-8 -*-
"""Reddit_DC_EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XK53wPY_e_3VdXtSAgsWKFS4djp-6mz7

# Data Cleaning and Exploratory Data Analysis
The raw training dataset has been generated from Subreddits using PRAW and saved as a CSV file named - SAD_subreddit_Post_Data.csv.

### Step 0: Load the essential Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import re
import numpy as np
import pandas as pd
import seaborn as sns
import missingno as msno
import matplotlib.pyplot as plt
# %matplotlib inline
import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
!pip install texthero  
import texthero as hero
from texthero import preprocessing
from texthero import stopwords
import plotly.express as px
from PIL import Image

"""###Step 1: Import the training data, Reshape the data and Fix the Missing data"""

# Import the generated Data
from google.colab import files
data_to_load = files.upload()

# Import the generated Data
SAD_Subreddit_Data = pd.read_csv('SAD_Subreddit_Data.csv')

# Viewing a glimpse of the Data
SAD_Subreddit_Data.head()

# Discarding the not so useful columns
SAD_Subreddit_Data = SAD_Subreddit_Data.drop(['ID', 'Author', 'Score', 'Comments','Post Count'],axis=1)

# Renaming the Post Category names so as to suit convention (Priorly held the name of the corresponding subreddit topic)
SAD_Subreddit_Data['Post Category'] = SAD_Subreddit_Data['Post Category'].replace(['depressed'],'Depression')
SAD_Subreddit_Data['Post Category'] = SAD_Subreddit_Data['Post Category'].replace(['SuicideWatch'],'Suicidal')
SAD_Subreddit_Data.head()

SAD_Subreddit_Data.info() ##check data type and other info

# Check the Missingness
SAD_Subreddit_Data.isnull().sum()

msno.matrix(SAD_Subreddit_Data)

msno.heatmap(SAD_Subreddit_Data)

# Addressing the Missing values
SAD_Subreddit_Data = SAD_Subreddit_Data[SAD_Subreddit_Data['Title'].notna()]
SAD_Subreddit_Data = SAD_Subreddit_Data[SAD_Subreddit_Data['Created DTTM'].notna()]
SAD_Subreddit_Data = SAD_Subreddit_Data[SAD_Subreddit_Data['Post Category'].notna()]
SAD_Subreddit_Data['Post'].fillna('  ',inplace=True)

# Re-Check the Missingness after changes
SAD_Subreddit_Data.isnull().sum()

# Viewing a glimpse of the Data
SAD_Subreddit_Data.head()

"""### Step 2:Training Data Pre-Processing"""

# Using Texthero to do the following changes in one go
# 1. fillna(s) Replace not assigned values with empty spaces.
# 2. lowercase(s) Lowercase all text.
# 3. remove_digits() Remove all blocks of digits.
# 4. remove_punctuation() Remove all string.punctuation (!'#$%&'()*+,-./:;<=>?@[\]^_`{|}~).
# 5. remove_diacritics() Remove all accents from strings.
# 6. remove_stopwords() Remove all stop words.
# 7. remove_whitespace() Remove all white space between words.
Stopwords = stopwords.DEFAULT
SAD_Stopwords = Stopwords.union(set(['aot', 'aos', 'aoll',  'aom', 'aove', 'put', 'te', 'r','want','feel','like','know']))
#added stop word like,feel, know that are common across multiple categories
#create a custom cleaning pipeline
DC_Pipeline = [preprocessing.fillna
               , preprocessing.lowercase
               , preprocessing.remove_digits
               , preprocessing.remove_punctuation
               , preprocessing.remove_diacritics
               , preprocessing.remove_whitespace
               , preprocessing.remove_urls
               #, preprocessing.stem
              ]
SAD_Subreddit_Data.insert(4, 'Clean Title', hero.clean(SAD_Subreddit_Data['Title'], pipeline = DC_Pipeline))
SAD_Subreddit_Data.insert(5, 'Clean Post', hero.clean(SAD_Subreddit_Data['Post'], pipeline = DC_Pipeline))
SAD_Subreddit_Data['Clean Title'] = hero.remove_stopwords(SAD_Subreddit_Data['Clean Title'], stopwords = SAD_Stopwords)
SAD_Subreddit_Data['Clean Post'] = hero.remove_stopwords(SAD_Subreddit_Data['Clean Post'], stopwords = SAD_Stopwords)
SAD_Subreddit_Data.tail()

# Combine Clean Title and Clean Post as a single column
SAD_Subreddit_Data.insert(4, 'Clean Text', SAD_Subreddit_Data['Clean Title'] + SAD_Subreddit_Data['Clean Post'])

# Discard Clean Title and Clean Post
SAD_Subreddit_Data = SAD_Subreddit_Data.drop(['Clean Title', 'Clean Post'],axis=1)
SAD_Subreddit_Data.head()

"""### Step 3: Training Data - Exploratory Data Analysis"""

# Pie Chart 1: Overview of data split based on Post Categories  
explode = [0.1,0,0]
labels = ['Anxiety', 'Depression', 'Suicidal']
SAD_Subreddit_Data['Post Category'].value_counts(normalize=True).plot.pie(labels = labels, explode = explode, shadow = True, autopct='%1.1f%%',figsize=(10,10), cmap = 'Purples_r')
plt.axes().set_ylabel('')
plt.tight_layout()
plt.show()

# Top 20 Words of user text
TopWords = hero.visualization.top_words(SAD_Subreddit_Data['Clean Text']).head(25)
TopWords.plot.bar(title = "Top Words", cmap = 'Purples_r', figsize = [15,10], );
plt.show(block=True);

# Wordcloud for All Post Categories
MaskAllCategories = np.array(Image.open('AllCategories.png'))
hero.wordcloud(SAD_Subreddit_Data['Clean Text'], contour_width= 1, max_words= 200, mask = MaskAllCategories, colormap = 'Purples_r', background_color = 'Black', width=4000, height=2500)

# Wordcloud for observations for "Anxiety" posts
MaskAnxiety = np.array(Image.open('Anxiety.png'))
Anxiety_Subreddit_Data = SAD_Subreddit_Data[SAD_Subreddit_Data['Post Category'] == 'Anxiety'] 
hero.wordcloud(Anxiety_Subreddit_Data['Clean Text'], max_words= 200, contour_width= 1, mask = MaskAnxiety, colormap = 'Purples_r', background_color = 'Black', width=4000, height=2500)

# Wordcloud for observations for "Depression" posts
MaskDepression = np.array(Image.open('Depression.png'))
Depression_Subreddit_Data = SAD_Subreddit_Data[SAD_Subreddit_Data['Post Category'] == 'Depression'] 
hero.wordcloud(Depression_Subreddit_Data['Clean Text'], max_words= 200, contour_width= 1, mask = MaskDepression, colormap = 'Purples_r', background_color = 'Black', width=4000, height=2500)

# Wordcloud for observations for "Suicidal" posts
MaskSuicidal = np.array(Image.open('Suicidal.jpg'))
Suicidal_Subreddit_Data = SAD_Subreddit_Data[SAD_Subreddit_Data['Post Category'] == 'Suicidal'] 
hero.wordcloud(Suicidal_Subreddit_Data['Clean Text'], max_words= 200, contour_width= 1, mask = MaskSuicidal, colormap = 'Purples_r', background_color = 'Black', width=4000, height=2500)

# Save the Data as CSV
SAD_Subreddit_Data.to_csv("SAD_Subreddit_Data_Cleaned.csv")

# To Download the Cleaned CSV file
from google.colab import files
files.download('SAD_Subreddit_Data_Cleaned.csv')

"""### Step 4: Feature Extraction on Training Data"""

from google.colab import files
data_to_load = files.upload()

# Import the cleaned Data
SAD_Subreddit_Data_Cleaned = pd.read_csv('SAD_Subreddit_Data_Cleaned .csv')

# Viewing a glimpse of the Data
SAD_Subreddit_Data_Cleaned.head()

SAD_Subreddit_Data_Cleaned = SAD_Subreddit_Data_Cleaned.drop(SAD_Subreddit_Data_Cleaned.columns[1],axis=1) ##Deleted unwanted Column

# Import Required libraries
!pip install -U textblob
from textblob import TextBlob

# Using TextBlob to generate the polarity and subjectivity scores for the text
polarity = lambda t: TextBlob(t).sentiment.polarity
subjectivity = lambda t: TextBlob(t).sentiment.subjectivity

SAD_Subreddit_Data_Cleaned['Polarity'] = SAD_Subreddit_Data_Cleaned['Clean Text'].apply(polarity)
SAD_Subreddit_Data_Cleaned['Subjectivity'] = SAD_Subreddit_Data_Cleaned['Clean Text'].apply(subjectivity)

SAD_Subreddit_Data_Cleaned.head()

##To Encode the Categories (Anxiety = 0, Depression = 1, Suicidal =2  )
from sklearn.preprocessing import OrdinalEncoder

ord_enc = OrdinalEncoder()
SAD_Subreddit_Data_Cleaned["category_code"] = ord_enc.fit_transform(SAD_Subreddit_Data_Cleaned[["Post Category"]])
SAD_Subreddit_Data_Cleaned["category_code"] = SAD_Subreddit_Data_Cleaned["category_code"].astype(int)
SAD_Subreddit_Data_Cleaned.head(5)

"""### Step 5: Download Training Data"""

SAD_Subreddit_Data_Cleaned.to_csv("SAD_Subreddit_train_Data.csv")

# Uncomment the following 2 lines Download the CSV file of training dataset
from google.colab import files
files.download('SAD_Subreddit_train_Data.csv')