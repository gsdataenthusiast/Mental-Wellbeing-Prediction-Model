# -*- coding: utf-8 -*-
"""Classification_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DgjnQMtYLyfyCWXE_pwkan1WKIcE7Kdt

## Fine Tuned Classifier Model

### Step 0: Load the essential Libraries and the Training Data
"""

# Commented out IPython magic to ensure Python compatibility.

import re
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

# Import the training Data (Cleaned and processed)
from google.colab import files
data_to_load = files.upload()

# Import the generated Data
SAD_Subreddit_train_Data = pd.read_csv('SAD_Subreddit_train_Data.csv')



# Viewing a glimpse of the Data
SAD_Subreddit_train_Data.head()

SAD_Subreddit_train_Data = SAD_Subreddit_train_Data.drop(SAD_Subreddit_train_Data.columns[0],axis=1) ##Deleted unwanted Column
# Viewing a glimpse of the Data
SAD_Subreddit_train_Data.head()

"""## Step 1:Feature and Label Selection"""

feature = SAD_Subreddit_train_Data['Clean Text'].values
label = SAD_Subreddit_train_Data['Post Category'].values

"""### Step 2: Train_test_split for splitting Traaining data and validation data"""

from sklearn.model_selection import train_test_split
trainX, testX, trainY, testY = train_test_split(feature, label, test_size=0.20, random_state= None)

"""###Step 3: Count Vectorization and Tf-IDF Transformation"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

count_vect = CountVectorizer(ngram_range=(1,3),min_df=0,max_df=1.0)

X_train_counts = count_vect.fit_transform(trainX)
count_vect.vocabulary_

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

X_test_counts = count_vect.transform(testX)
count_vect.vocabulary_

X_test_tfidf = tfidf_transformer.transform(X_test_counts)

"""###Step 4: Fit the Classifier Model"""

from sklearn.svm import SVC
svc = SVC(kernel='linear')
svm = svc.fit(X_train_tfidf, trainY)

"""###Step 5: Predict on validation set"""

predY = svm.predict(X_test_tfidf)

"""###Step 6: Evaluate Model Performance"""

##Classification Report
from sklearn import metrics
print(metrics.classification_report(testY,predY))

import matplotlib.pyplot as plt  
from sklearn.metrics import plot_confusion_matrix

##Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(testY,predY)

##Plot Confusion Matrix
import seaborn as sns
import matplotlib.pyplot as plt     

ax= plt.subplot()
sns.heatmap(cm,annot = True,fmt="d", ax = ax, cmap='Purples'); #annot=True to annotate cells
sns.cubehelix_palette(as_cmap=True)

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Anxiety', 'Depression','Suicidal']); ax.yaxis.set_ticklabels(['Anxiety', 'Depression','Suicidal']);

"""### Step 7: Classify Test Data"""

# Import the test Data (Cleaned and processed)
from google.colab import files
data_to_load = files.upload()

Riposte_MWB_Data = pd.read_csv('Riposte_MWB_Data.csv')
# Viewing a glimpse of the Data
Riposte_MWB_Data.info()

Riposte_MWB_Data = Riposte_MWB_Data[Riposte_MWB_Data['Cleaned Topic Hashtags'].notna()]

Riposte_MWB_Data = Riposte_MWB_Data.drop(Riposte_MWB_Data.columns[0],axis=1) ##Deleted unwanted Column
# Viewing a glimpse of the Data
Riposte_MWB_Data.head()

Merged1 = SAD_Subreddit_train_Data[['Clean Text','Post Category','Polarity','Subjectivity']]
Merged1.head()

Merged2 = pd.DataFrame(Riposte_MWB_Data[['Cleaned Topic Hashtags','Polarity','Subjectivity']])
Merged2['Clean Text'] = Merged2['Cleaned Topic Hashtags']
del Merged2['Cleaned Topic Hashtags']
Merged2.head(5)

Merged = Merged1.append(Merged2, ignore_index=True)
Merged.shape

k = Merged1.shape[0] 
k

train_merged = Merged.iloc[:k]
train_merged.shape[0]

pred_merged = Merged.iloc[k:]
pred_merged.shape[0]

Subjectivity,Polarity = (pred_merged['Subjectivity'].values,pred_merged['Polarity'].values)

X_train, y_train = (train_merged['Clean Text'].values, train_merged['Post Category'].values)
X_train

Px_test, Py_test = (pred_merged['Clean Text'].values, pred_merged['Post Category'].values)
Px_test

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

count_vect = CountVectorizer(ngram_range=(1,3),min_df=0,max_df=1.0)

X_train_counts = count_vect.fit_transform(X_train)
count_vect.vocabulary_

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

Px_test_counts = count_vect.transform(Px_test).astype('U')
count_vect.vocabulary_

Px_test_tfidf = tfidf_transformer.transform(Px_test_counts)

from sklearn.svm import SVC
svc = SVC(kernel='linear')
svm = svc.fit(X_train_tfidf, y_train)

predictions = svm.predict(Px_test_tfidf)

df = pd.DataFrame(columns=["Hashtags", "Predictions","Subjectivity","Polarity"])

for i in range(len(pred_merged)):
	#print(Px_test[i],Px_test_tfidf[i], predictions[i])
   df.loc[i] = Px_test[i] + predictions[i]
   df.loc[i] = {'Hashtags':Px_test[i], 'Predictions':predictions[i], 'Subjectivity':Subjectivity[i],'Polarity':Polarity[i]}

df.head(5)

df.to_csv('Riposte_predictions_Result.csv')
from google.colab import files
files.download("Riposte_predictions_Result.csv")