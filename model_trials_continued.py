# -*- coding: utf-8 -*-
"""Model Trials Continued.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14UQVO_7lX1dc7JmFJC1maSp4s8skRm6u
"""

# Commented out IPython magic to ensure Python compatibility.
import re
import numpy as np
import pandas as pd
import seaborn as sns
import missingno as msno
import matplotlib.pyplot as plt
# %matplotlib inline
import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
!pip install texthero  
import texthero as hero
from texthero import preprocessing
from texthero import stopwords
import plotly.express as px
from PIL import Image

# Import the generated Data
from google.colab import files
data_to_load = files.upload()

# Import the generated Data

SAD_Subreddit_Data = pd.read_csv('SAD_Subreddit_Data.csv')

# Viewing a glimpse of the Data
SAD_Subreddit_Data.head()
#pd.set_option('display.max_rows', SAD_Subreddit_Data.shape[0]+1)
#print(SAD_Subreddit_Data)

# Discarding the not so useful columns
SAD_Subreddit_Data = SAD_Subreddit_Data.drop(['ID', 'Author', 'Score', 'Comments'],axis=1)

# Renaming the Post Category names so as to suit convention (Priorly held the name of the corresponding subreddit topic)
SAD_Subreddit_Data['Post Category'] = SAD_Subreddit_Data['Post Category'].replace(['depressed'],'Depression')
SAD_Subreddit_Data['Post Category'] = SAD_Subreddit_Data['Post Category'].replace(['SuicideWatch'],'Suicidal')
SAD_Subreddit_Data.head()

# Addressing the Missing values
SAD_Subreddit_Data = SAD_Subreddit_Data[SAD_Subreddit_Data['Post Count'].notna()]
SAD_Subreddit_Data = SAD_Subreddit_Data[SAD_Subreddit_Data['Title'].notna()]
SAD_Subreddit_Data = SAD_Subreddit_Data[SAD_Subreddit_Data['Created DTTM'].notna()]
SAD_Subreddit_Data = SAD_Subreddit_Data[SAD_Subreddit_Data['Post Category'].notna()]
SAD_Subreddit_Data['Post'].fillna('  ',inplace=True)

#Check the Missingness after changes
SAD_Subreddit_Data.isnull().sum()

DC_Pipeline = [preprocessing.fillna
               , preprocessing.lowercase
               , preprocessing.remove_digits
               , preprocessing.remove_punctuation
               , preprocessing.remove_diacritics
               , preprocessing.remove_whitespace
               , preprocessing.remove_urls
              #, preprocessing.stem
              ]

SAD_Subreddit_Data.insert(4, 'Clean Title', hero.clean(SAD_Subreddit_Data['Title'], pipeline = DC_Pipeline))
SAD_Subreddit_Data.insert(5, 'Clean Post', hero.clean(SAD_Subreddit_Data['Post'], pipeline = DC_Pipeline))
SAD_Subreddit_Data.tail()

# Combine Clean Title and Clean Post as a single column
#SAD_Subreddit_Data.insert(4, 'Clean Text', SAD_Subreddit_Data['Clean Title'] +SAD_Subreddit_Data['Clean Post'])

SAD_Subreddit_Data['Clean Text'] = SAD_Subreddit_Data['Clean Title'].str.cat(SAD_Subreddit_Data['Clean Post'],sep=" ")
# Discard Clean Title and Clean Post
SAD_Subreddit_Data = SAD_Subreddit_Data.drop(['Clean Title', 'Clean Post'],axis=1)
SAD_Subreddit_Data.head(20)

# Import Required libraries
!pip install -U textblob
from textblob import TextBlob

# Using TextBlob to generate the polarity and subjectivity scores for the text
polarity = lambda t: TextBlob(t).sentiment.polarity
subjectivity = lambda t: TextBlob(t).sentiment.subjectivity

SAD_Subreddit_Data['Polarity'] = SAD_Subreddit_Data['Clean Text'].apply(polarity)
SAD_Subreddit_Data['Subjectivity'] = SAD_Subreddit_Data['Clean Text'].apply(subjectivity)

SAD_Subreddit_Data.head()

##Test Data cleaning 
!pip install wordninja
import wordninja

from google.colab import files
data_to_load = files.upload()

# Import the Mental Wellbeing Test Data
Riposte_MWB_Data = pd.read_csv('Riposte_Posts_1_25_2021, 12_42_08 PM For testing mental health model (1).csv')
# Viewing a glimpse of the Data
Riposte_MWB_Data.head()

# Clean the Text Column
# Step 1: Apply the Default TextHero Pipeline
# Step 2: Remove Stopwords
# Step 3: Split the hashtags in words

DC_Pipeline = [preprocessing.fillna
               , preprocessing.lowercase
               , preprocessing.remove_digits
               , preprocessing.remove_punctuation
               , preprocessing.remove_diacritics
               , preprocessing.remove_whitespace
               , preprocessing.remove_urls
              #, preprocessing.stem
              ]

Riposte_MWB_Data.insert(9, 'Cleaned Topic Hashtags', hero.clean(Riposte_MWB_Data['Topic Hashtags'], pipeline = DC_Pipeline))

Riposte_MWB_Data['Cleaned Topic Hashtags'] = Riposte_MWB_Data['Cleaned Topic Hashtags'].apply(lambda x: ' '.join(wordninja.split(x)))
Riposte_MWB_Data.head()

# Import Required libraries
!pip install -U textblob
from textblob import TextBlob

# Using TextBlob to generate the polarity and subjectivity scores for the text
polarity = lambda t: TextBlob(t).sentiment.polarity
subjectivity = lambda t: TextBlob(t).sentiment.subjectivity

Riposte_MWB_Data['Polarity'] = Riposte_MWB_Data['Cleaned Topic Hashtags'].apply(polarity)
Riposte_MWB_Data['Subjectivity'] = Riposte_MWB_Data['Cleaned Topic Hashtags'].apply(subjectivity)
Riposte_MWB_Data.head(5)

##Filtering only "Face Palm" to test the classifier
Riposte_MWB_Data = Riposte_MWB_Data.loc[Riposte_MWB_Data['Post Type'] == 'Facepalm']
Riposte_MWB_Data.shape

train_vector = SAD_Subreddit_Data['Clean Text'].values
pred_vector = Riposte_MWB_Data['Cleaned Topic Hashtags'].values

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

count_vect = CountVectorizer()

X_train_counts = count_vect.fit_transform(train_vector)
count_vect.vocabulary_

tfidf_transformer = TfidfTransformer()
trainXifdf = tfidf_transformer.fit_transform(X_train_counts)

Px_test_counts = count_vect.transform(pred_vector)
count_vect.vocabulary_

predXifdf = tfidf_transformer.transform(Px_test_counts)

y_train = SAD_Subreddit_Data['Post Category'].values

from sklearn.svm import SVC
svc = SVC(kernel='linear')
svm_1 = svc.fit(trainXifdf, y_train)

predictions = svm_1.predict(predXifdf)

df = pd.DataFrame(columns=["Hashtags", "Predictions","Polarity","Subjectivity"])

for i in range(len(Riposte_MWB_Data)):
	#print(Px_test[i],Px_test_tfidf[i], predictions[i])
   df.loc[i] = pred_vector[i] + predictions[i] +polarity
   df.loc[i] = {'Hashtags':pred_vector[i], 'Predictions':predictions[i]}

df.head(5)

polarity = lambda t: TextBlob(t).sentiment.polarity
subjectivity = lambda t: TextBlob(t).sentiment.subjectivity

df['Polarity'] = df['Hashtags'].apply(polarity)
df['Subjectivity'] = df['Hashtags'].apply(subjectivity)

df.head()

df.to_csv('predictionswithoutstopword.csv')
from google.colab import files
files.download("predictionswithoutstopword.csv")

##2nd model

feature_train = SAD_Subreddit_Data[['Subjectivity', 'Polarity']].values
feature_test = Riposte_MWB_Data[['Subjectivity','Polarity']].values
label = SAD_Subreddit_Data['Post Category'].values

from sklearn.model_selection import train_test_split
trainX, testX, trainY, testY = train_test_split(feature_train,label, test_size=0.20, random_state= 0 , stratify=label)

from sklearn.svm import SVC
svc = SVC(kernel='linear')
svm_2 = svc.fit(trainX,trainY)

y_pred = svm.predict(testX)

from sklearn import metrics
print(metrics.classification_report(testY,y_pred))

